# Lecture 11. Neural Network Fundamentals 


**1. Deep Learning Topics:**
- Fundamentals of Neural Networks
- Training using gradient backpropagation
- Regularization techniques
- Network architectures: Autoencoders, Convolutional networks (CNN), Recurrent networks (RNNs), Attention and Transformers.

**2. Multi-layer Perceptron:**
- Introduces non-linearity through function composition.

**3. Perceptron Model:**
- Basic building block for ANNs.
- Consists of inputs, synaptic weights, bias weight, and an activation function.

**4. Limitations of Linear Models:**
- Many real-world problems aren't linearly separable.
- XOR problem introduced as an example.

**5. Activation Functions in ANNs:**
- Step function, Sign function, Logistic function, tanh function, Rectified linear unit (ReLU), etc.

**6. Feed-forward Artificial Neural Network:**
- Comprises input layer, hidden layer(s), and output layer.

**7. ANN in Supervised Learning:**
- Can handle univariate regression, multivariate regression, binary classification, and multiclass classification.

**8. Power of ANNs:**
- Capable of approximating various non-linear functions.
- Universal Approximation Theorem: Single-layer ANNs can approximate any continuous function.

**9. Deep Learning and Representation Learning:**
- Depth refers to the number of hidden layers.
- Consecutive layers in ANNs form representations of increasing complexity.
- Depth often provides more accurate models than width alone.

**10. Backpropagation:**
- A method to calculate the gradient of loss in a neural network.
- Uses the chain rule for derivatives.
- Consists of forward and backward propagation of errors.
- Helps in updating weights based on the error.

**11. Next Topics:**
- Deep net training and autoencoders.

---

Note: The above summary captures the core concepts and topics discussed in the lecture. It is advisable to revisit the detailed content for a deeper understanding, especially if specific examples, equations, or nuances are essential for your application.