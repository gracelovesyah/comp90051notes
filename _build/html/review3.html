

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Review 3 &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'review3';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Review 2" href="review2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to COMP90051
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="revision_progress.html">Revision Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="basics.html">Basic Concepts</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week1.0.html">week1</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week1.1.html">Lecture 1.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.2.html">Lecture 2.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet2note.html">worksheet2note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week2.0.html">week2</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week2.1.html">Lecture 3.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.2.html">Lecture 4.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet3note.html">worksheet3note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week3.0.html">week3</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week3.1.html">Lecture 5.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.2.html">Lecture 6.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet4note.html">worksheet4note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week4.0.html">week4</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week4.1.html">Lecture 7.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.2.html">Lecture 8.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet5note.html">worksheet5note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week5.0.html">week5</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week5.1.html">Lecture 9.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.2.html">Lecture 10.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet6note.html">worksheet6note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week6.0.html">week6</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week6.1.html">Lecture 11.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week6.2.html">Lecture 12.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week6.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week7.0.html">week7</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week7.1.html">Lecture 13. Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.2.html">Lecture 14. RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.3.html">Additional Resource</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week8.0.html">week8</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week8.1.html">Lecture 16 Graph Convolution Networks (Deep Learning After You Drop The Camera)</a></li>
<li class="toctree-l2"><a class="reference internal" href="week8.2.html">Lecture 16. Learning with expert advice</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week9.0.html">week9</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week9.1.html">Stochastic Multi-Armed Bandits (MABs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="week9.2.html">Bayesian regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet10.html">Workshop 10: Multi-armed bandits</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet10note.html">Workshop 10: Multi-armed bandits notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week10.0.html">week10</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week10.1.html">wk10 notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="week10.2.html">PGM Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="week10.3.html">Additional Notes -  More on Bayesian</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet11.html">COMP90051 Workshop 11</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week11.0.html">week11</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week11.1.html">U-PGM</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.2.html">SVM assignment</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.3.html">Lecture 22. Inference on PGMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="asm1feedback.html">ASM2 feedback</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week12.0.html">week12</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week12.1.html">Lecture 22. Inference on PGMs Cont. &amp; Lecture 23. Gaussian Mixture Models</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="review.html">Review Notes</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="review0.html">Review 0</a></li>
<li class="toctree-l2"><a class="reference internal" href="review1.html">Review 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="review2.html">Review 2</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Review 3</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Freview3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/review3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Review 3</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#toc">ToC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equation">normal equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-raphston">Newton Raphston</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca">PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-learning">PAC Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-learnability">PAC Learnability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-cont">PAC Cont.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vc-dimension">VC dimension</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#def">Def</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#addtional-resource">Addtional Resource</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="review-3">
<h1>Review 3<a class="headerlink" href="#review-3" title="Permalink to this heading">#</a></h1>
<p>This notes is completed with assistance of <a class="reference external" href="https://chat.openai.com/c/20c2aa50-1af7-489f-8cf6-3407f4264c37">ChatGPT</a></p>
<section id="toc">
<h2>ToC<a class="headerlink" href="#toc" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Iterative optimization</p></li>
<li><p>gradient descent, Newton Raphson</p></li>
<li><p>Regularization</p></li>
<li><p>PAC, VC dimension</p></li>
</ul>
</section>
<section id="normal-equation">
<h2>normal equation<a class="headerlink" href="#normal-equation" title="Permalink to this heading">#</a></h2>
<p>The normal equation is a method used to find the optimal parameters for a linear regression model without resorting to iterative optimization techniques like gradient descent. If you have a design matrix <span class="math notranslate nohighlight">\( X \)</span> (with each row being a training example and each column being a feature) and a vector <span class="math notranslate nohighlight">\( y \)</span> of target values, the normal equation provides a formula to directly compute the optimal weight vector <span class="math notranslate nohighlight">\( \theta \)</span> for the linear regression model.</p>
<p>The formula for the normal equation is:</p>
<div class="math notranslate nohighlight">
\[ \theta = (X^T X)^{-1} X^T y \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \theta \)</span> is the vector of model parameters.</p></li>
<li><p><span class="math notranslate nohighlight">\( X \)</span> is the design matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\( y \)</span> is the vector of target values.</p></li>
<li><p><span class="math notranslate nohighlight">\( X^T \)</span> is the transpose of <span class="math notranslate nohighlight">\( X \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( (X^T X)^{-1} \)</span> is the inverse of the matrix <span class="math notranslate nohighlight">\( X^T X \)</span>.</p></li>
</ul>
<p>This equation gives you the parameter values that minimize the cost function for linear regression. However, it’s worth noting that the normal equation can be computationally expensive for very large datasets or when <span class="math notranslate nohighlight">\( X^T X \)</span> is non-invertible (singular). In such cases, iterative methods like gradient descent might be more appropriate.</p>
</section>
<section id="newton-raphston">
<h2>Newton Raphston<a class="headerlink" href="#newton-raphston" title="Permalink to this heading">#</a></h2>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>“Recall that linear regression’s objective can be written as <span class="math notranslate nohighlight">\( L(\theta) = \frac{1}{2} ||X\theta - y||^2 \)</span>, where <span class="math notranslate nohighlight">\( X \)</span> is a <span class="math notranslate nohighlight">\( n \times d \)</span> matrix of training instances in rows, <span class="math notranslate nohighlight">\( y \in \mathbb{R}^n \)</span> are the training labels, and <span class="math notranslate nohighlight">\( \theta \in \mathbb{R}^d \)</span> are the parameters to learn by minimising <span class="math notranslate nohighlight">\( L(\theta) \)</span>. Show that for any starting <span class="math notranslate nohighlight">\( \theta \in \mathbb{R}^d \)</span>, a single step of Newton-Raphson optimisation of <span class="math notranslate nohighlight">\( L(\theta) \)</span> recovers the normal equation exact solution. (Hints for taking vector/matrix-valued derivatives with respect to <span class="math notranslate nohighlight">\( \theta \)</span>, for constants matrix <span class="math notranslate nohighlight">\( A \)</span> and vector <span class="math notranslate nohighlight">\( b \)</span>. Hint 1: <span class="math notranslate nohighlight">\( \nabla (A\theta + b) = A \)</span>. Hint 2: <span class="math notranslate nohighlight">\( \nabla^2(A\theta) = A \)</span>. Hint 3: <span class="math notranslate nohighlight">\( \nabla ||f (\theta)||^2 \)</span> = <span class="math notranslate nohighlight">\( 2 \nabla f(\theta)^T f(\theta) \)</span>. Where prime ‘ denotes transpose, <span class="math notranslate nohighlight">\( \nabla \)</span> the gradient, and <span class="math notranslate nohighlight">\( \nabla^2 \)</span> the Hessian as in class, here always with respect to <span class="math notranslate nohighlight">\( \theta \)</span>).”</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>Alright, let’s correct that and go through the process again.</p>
<p>Given the objective function:</p>
<div class="math notranslate nohighlight">
\[ L(\theta) = \frac{1}{2} ||X\theta - y||^2 \]</div>
<p>We want to show that a single step of the Newton-Raphson optimization recovers the normal equation solution.</p>
<p>The Newton-Raphson update rule for optimization is:</p>
<div class="math notranslate nohighlight">
\[ \theta_{\text{new}} = \theta_{\text{old}} - \left( \nabla^2 L(\theta_{\text{old}}) \right)^{-1} \nabla L(\theta_{\text{old}}) \]</div>
<p><strong>Step 1:</strong> Compute the gradient <span class="math notranslate nohighlight">\( \nabla L(\theta) \)</span>.</p>
<p>Using the given hints:</p>
<div class="math notranslate nohighlight">
\[ \nabla (A\theta + b) = A \]</div>
<div class="math notranslate nohighlight">
\[ \nabla ||f(\theta)||^2 = 2 f(\theta)^T \nabla f(\theta) \]</div>
<p>The gradient is:</p>
<div class="math notranslate nohighlight">
\[ \nabla L(\theta) = \frac{1}{2} \times 2 (X\theta - y)^T X \]</div>
<div class="math notranslate nohighlight">
\[ \nabla L(\theta) = X^T (X\theta - y) \]</div>
<p><strong>Step 2:</strong> Compute the Hessian <span class="math notranslate nohighlight">\( \nabla^2 L(\theta) \)</span>.</p>
<p>Using the hint:</p>
<div class="math notranslate nohighlight">
\[ \nabla^2 (A\theta) = A \]</div>
<p>The Hessian is:</p>
<div class="math notranslate nohighlight">
\[ \nabla^2 L(\theta) = X^T X \]</div>
<p><strong>Step 3:</strong> Plug into the Newton-Raphson update rule.</p>
<div class="math notranslate nohighlight">
\[ \theta_{\text{new}} = \theta_{\text{old}} - (X^T X)^{-1} X^T (X\theta_{\text{old}} - y) \]</div>
<p>Notice that if <span class="math notranslate nohighlight">\( \theta_{\text{old}} \)</span> is any arbitrary starting point, then <span class="math notranslate nohighlight">\( \theta_{\text{new}} \)</span> will be the solution of the normal equation:</p>
<div class="math notranslate nohighlight">
\[ \theta = (X^T X)^{-1} X^T y \]</div>
<p>This shows that a single step of Newton-Raphson optimization recovers the normal equation exact solution.</p>
</div>
</section>
<section id="pca">
<h2>PCA<a class="headerlink" href="#pca" title="Permalink to this heading">#</a></h2>
<p>The formula you provided breaks down the excess risk of a learned model <span class="math notranslate nohighlight">\( f_m \)</span> into two components: the estimation error and the approximation error. Let’s dissect the formula and its components:</p>
<ol class="arabic simple">
<li><p><strong>Excess Risk</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[f_m] - R^* \]</div>
<p>This represents the difference in risk (or error) between the model <span class="math notranslate nohighlight">\( f_m \)</span> that we’ve learned from our data and the optimal model with the lowest possible risk <span class="math notranslate nohighlight">\( R^* \)</span>. This difference is what we refer to as the “excess risk” of <span class="math notranslate nohighlight">\( f_m \)</span>.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Estimation Error</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[f_m] - R[f^*] \]</div>
<p>This is the difference in risk between the model <span class="math notranslate nohighlight">\( f_m \)</span> and the best possible model <span class="math notranslate nohighlight">\( f^* \)</span> from our hypothesis class. The estimation error arises because of the limitations of our learning algorithm and the finite amount of data we have. It captures the error due to the “estimation” process of learning <span class="math notranslate nohighlight">\( f_m \)</span> from data.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Approximation Error</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[f^*] - R^* \]</div>
<p>This represents the difference in risk between the best possible model <span class="math notranslate nohighlight">\( f^* \)</span> from our hypothesis class and the optimal model with the lowest possible risk <span class="math notranslate nohighlight">\( R^* \)</span>. The approximation error arises because our hypothesis class might not contain the optimal model. It captures the error due to the “approximation” of the true underlying function by the best function in our hypothesis class.</p>
<p>To summarize:</p>
<ul class="simple">
<li><p><strong>Excess Risk</strong>: The total additional risk our learned model <span class="math notranslate nohighlight">\( f_m \)</span> has compared to the optimal model.</p></li>
<li><p><strong>Estimation Error</strong>: The error due to the process of learning a model from data.</p></li>
<li><p><strong>Approximation Error</strong>: The error due to the limitations of our hypothesis class in approximating the true underlying function.</p></li>
</ul>
<p>The formula essentially decomposes the excess risk into these two main sources of error, helping us understand where the shortcomings of our learned model come from.</p>
<section id="pac-learning">
<h3>PAC Learning<a class="headerlink" href="#pac-learning" title="Permalink to this heading">#</a></h3>
<p><strong>What</strong>: A framework for the mathematical analysis of machine learning</p>
<p><strong>Goal</strong>:</p>
<ul class="simple">
<li><p>The goal is with high probability, the selected hypothesis will have lower error.</p></li>
<li><p>AKA achieve low generalization error with high prob.</p></li>
<li><p>AKA we want our model to be approximately correct as much as possible where</p>
<ul>
<li><p>approximately correct is up-bounded by the error rate (<span class="math notranslate nohighlight">\(\epsilon\)</span>) and</p></li>
<li><p>as much as possible is low-bounded by <span class="math notranslate nohighlight">\(delta\)</span></p></li>
<li><p>where both <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span> are very small numbers.
<strong>Parameters</strong>:
<span class="math notranslate nohighlight">\(\epsilon\)</span> gives an upper bound on the error in accuracy with which h approximated (accuracy: <span class="math notranslate nohighlight">\(1 - \epsilon\)</span>) <span class="math notranslate nohighlight">\(\delta\)</span> gives the probability of failure in achieving this accuracy (confidence: <span class="math notranslate nohighlight">\(1 - \delta\)</span>)</p></li>
</ul>
</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>In the PAC model, we specify two small parameters, <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(\delta\)</span>, and require that with probability at least <span class="math notranslate nohighlight">\(1 - \delta\)</span> a system learn a concept with error at most <span class="math notranslate nohighlight">\(\epsilon\)</span>.</p>
</div>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/pca1.png"><img alt="PCA" class="bg-primary mb-1 align-center" src="_images/pca1.png" style="width: 800px;" /></a>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/pca2.png"><img alt="PCA" class="bg-primary mb-1 align-center" src="_images/pca2.png" style="width: 800px;" /></a>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/pca3.png"><img alt="PCA" class="bg-primary mb-1 align-center" src="_images/pca3.png" style="width: 800px;" /></a>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/pca4.png"><img alt="PCA" class="bg-primary mb-1 align-center" src="_images/pca4.png" style="width: 800px;" /></a>
<p>That is if we want to have an accuracy of € and confidence of at least <span class="math notranslate nohighlight">\(1 - \delta\)</span> we have to choose a sample size m such that:</p>
<div class="math notranslate nohighlight">
\[
m &gt; \frac{4}{\epsilon}ln\frac{4}{\delta}
\]</div>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/pca5.png"><img alt="PCA" class="bg-primary mb-1 align-center" src="_images/pca5.png" style="width: 800px;" /></a>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=fTWm2S5tFCo">Probably Approximately Correct (PAC)Learning ( KTU CS467 Machine Learning Module 2)</a></p></li>
</ul>
</section>
<section id="pac-learnability">
<h3>PAC Learnability<a class="headerlink" href="#pac-learnability" title="Permalink to this heading">#</a></h3>
<p>A concept class <span class="math notranslate nohighlight">\( F \)</span> is said to be PAC learnable if there exists a polynomial-time learning algorithm such that for any target concept <span class="math notranslate nohighlight">\( f \)</span> in <span class="math notranslate nohighlight">\( F \)</span>, any distribution <span class="math notranslate nohighlight">\( D \)</span> over the input space, and for any <span class="math notranslate nohighlight">\( \epsilon, \delta &gt; 0 \)</span> (where <span class="math notranslate nohighlight">\( \epsilon \)</span> is the approximation error and <span class="math notranslate nohighlight">\( \delta \)</span> is the probability bound), the algorithm will, with probability at least <span class="math notranslate nohighlight">\( 1 - \delta \)</span>, output a hypothesis <span class="math notranslate nohighlight">\( h \)</span> whose error is at most <span class="math notranslate nohighlight">\( \epsilon \)</span>, provided it is given a number of samples <span class="math notranslate nohighlight">\( m \)</span> that is polynomial in <span class="math notranslate nohighlight">\( \frac{1}{\epsilon} \)</span> and <span class="math notranslate nohighlight">\( \frac{1}{\delta} \)</span>.</p>
<p>The notation <span class="math notranslate nohighlight">\( m = \mathcal{O}(\text{poly}(\frac{1}{\epsilon}, \frac{1}{\delta})) \)</span> means that the sample complexity <span class="math notranslate nohighlight">\( m \)</span> is bounded by a polynomial function of <span class="math notranslate nohighlight">\( \frac{1}{\epsilon} \)</span> and <span class="math notranslate nohighlight">\( \frac{1}{\delta} \)</span>.</p>
<p>However, the statement “if <span class="math notranslate nohighlight">\( m = 0 \)</span>” is confusing in this context. If <span class="math notranslate nohighlight">\( m \)</span> is 0, it means no samples are required, which doesn’t make sense in the PAC learning framework. It’s possible there’s a misunderstanding or miscommunication about the notation or concept. If you have a specific question or context about PAC learnability, please provide more details.</p>
<p><strong>Example</strong></p>
<p>Certainly! Let’s break down the concept of PAC (Probably Approximately Correct) learning into simpler terms.</p>
<p>Imagine you’re trying to teach a robot to recognize cats. You show the robot a bunch of pictures, some of cats and some of other things. After this training, you want the robot to correctly identify new pictures of cats that it hasn’t seen before.</p>
<p>Now, let’s relate this to the PAC learning framework:</p>
<ol class="arabic simple">
<li><p><strong>Concept Class <span class="math notranslate nohighlight">\( \mathcal{F} \)</span></strong>: This is like the set of all possible “rules” or “methods” the robot could use to identify cats. Each rule or method is a “concept”.</p></li>
<li><p><strong>Target Concept <span class="math notranslate nohighlight">\( f \)</span> in <span class="math notranslate nohighlight">\( \mathcal{F} \)</span></strong>: This is the “correct rule” for identifying cats. We assume there’s a right way to do it, even if we don’t know what it is.</p></li>
<li><p><strong>Distribution <span class="math notranslate nohighlight">\( \mathcal{D} \)</span> over the input space</strong>: This is like saying that some pictures are more likely to be shown to the robot than others. For example, maybe close-up pictures of cats are more common than distant ones.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\( \epsilon \)</span> (Approximation Error)</strong>: This is how much error we’re willing to tolerate. If <span class="math notranslate nohighlight">\( \epsilon \)</span> is 0.05, then we’re okay if the robot gets 5% of new cat pictures wrong.</p></li>
<li><p><strong><span class="math notranslate nohighlight">\( \delta \)</span> (Probability Bound)</strong>: This is our confidence level. If <span class="math notranslate nohighlight">\( \delta \)</span> is 0.01, then we want to be 99% sure that the robot’s error is at most <span class="math notranslate nohighlight">\( \epsilon \)</span>.</p></li>
<li><p><strong>PAC Learnable</strong>: A set of rules (or concepts) is PAC learnable if there’s a way (an algorithm) to teach the robot so that, after seeing a certain number of pictures, it can recognize new cat pictures with an error rate of at most <span class="math notranslate nohighlight">\( \epsilon \)</span> and we’re <span class="math notranslate nohighlight">\( 1-\delta \)</span> confident in this.</p></li>
<li><p><strong>Polynomial-time learning algorithm</strong>: This means that the time it takes for the robot to learn doesn’t grow too fast as we show it more pictures or demand higher accuracy.</p></li>
<li><p><strong>Sample Complexity <span class="math notranslate nohighlight">\( m \)</span></strong>: This is the number of pictures we need to show the robot for it to learn well. If our set of rules is PAC learnable, then the number of pictures (samples) we need to show the robot is reasonable (i.e., it’s not an astronomically large number).</p></li>
</ol>
<p>The notation <span class="math notranslate nohighlight">\( m = \mathcal{O}(\text{poly}(1/\epsilon, 1/\delta)) \)</span> is a mathematical way of saying that the number of pictures we need to show the robot grows in a “polynomial” way based on how accurate we want it to be (<span class="math notranslate nohighlight">\( \epsilon \)</span>) and how confident we want to be in its accuracy (<span class="math notranslate nohighlight">\( \delta \)</span>). In simpler terms, it means that if we want the robot to be more accurate or more confident, we might need to show it more pictures, but not an unreasonable amount more.</p>
<p>In essence, PAC learning is a framework that helps us understand when and how we can teach algorithms to generalize well from training data to new, unseen data.</p>
<hr class="docutils" />
<p>The exact number of pictures (or samples) you need to show depends on several factors:</p>
<ol class="arabic simple">
<li><p><strong>The Complexity of the Hypothesis Class</strong>: If the set of rules (concepts) the robot can use to identify cats is very complex, you might need more samples to ensure you’ve covered enough cases for the robot to generalize well.</p></li>
<li><p><strong>Desired Accuracy (<span class="math notranslate nohighlight">\( \epsilon \)</span>)</strong>: If you want the robot to be very accurate (low <span class="math notranslate nohighlight">\( \epsilon \)</span>), you’ll need more samples. For instance, if you want the robot to be 99% accurate, you’ll need more pictures than if you’re okay with it being 90% accurate.</p></li>
<li><p><strong>Confidence Level (<span class="math notranslate nohighlight">\( \delta \)</span>)</strong>: If you want to be very confident in the robot’s accuracy, you’ll need more samples. For example, if you want to be 99.9% sure of the robot’s performance, you’ll need more pictures than if you’re okay with being 95% sure.</p></li>
<li><p><strong>Distribution of the Data (<span class="math notranslate nohighlight">\( \mathcal{D} \)</span>)</strong>: If the pictures you have are very diverse and cover a wide range of cat scenarios (e.g., different breeds, lighting conditions, angles), you might need fewer samples than if your pictures are very similar.</p></li>
</ol>
<p>The notation <span class="math notranslate nohighlight">\( m = \mathcal{O}(\text{poly}(1/\epsilon, 1/\delta)) \)</span> gives a general idea of how the number of samples grows with <span class="math notranslate nohighlight">\( \epsilon \)</span> and <span class="math notranslate nohighlight">\( \delta \)</span>, but it doesn’t give an exact number. It says that the number of samples grows polynomially with <span class="math notranslate nohighlight">\( 1/\epsilon \)</span> and <span class="math notranslate nohighlight">\( 1/\delta \)</span>. In practice, the exact number also depends on the specific learning problem and the algorithm used.</p>
<p>For a more concrete answer in the context of PAC learning, one would typically derive a bound on the sample complexity based on the VC dimension of the hypothesis class and the desired <span class="math notranslate nohighlight">\( \epsilon \)</span> and <span class="math notranslate nohighlight">\( \delta \)</span>. This bound would give an upper limit on the number of samples required to achieve the desired accuracy and confidence. However, in real-world applications, it’s common to use empirical methods, like cross-validation, to determine an appropriate sample size or to check the performance of a model.</p>
</section>
<hr class="docutils" />
<section id="pac-cont">
<h3>PAC Cont.<a class="headerlink" href="#pac-cont" title="Permalink to this heading">#</a></h3>
<p>The equations you’ve provided are related to the PAC (Probably Approximately Correct) learning framework and the generalization bounds of learning algorithms. These bounds provide a theoretical guarantee on the performance of a learning algorithm on unseen data based on its performance on the training data. Let’s break down the concepts:</p>
<ol class="arabic simple">
<li><p><strong>True Risk of One Function</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[f] &lt; \hat{R}[f] + \sqrt{\frac{\log(1/\delta)}{2m}} \]</div>
<p>This equation provides a bound on the true risk (or error) <span class="math notranslate nohighlight">\( R[f] \)</span> of a function <span class="math notranslate nohighlight">\( f \)</span> in terms of its empirical risk <span class="math notranslate nohighlight">\( \hat{R}[f] \)</span> (i.e., its error on the training data) and the number of training samples <span class="math notranslate nohighlight">\( m \)</span>. The term <span class="math notranslate nohighlight">\( \sqrt{\frac{\log(1/\delta)}{2m}} \)</span> is a confidence interval that ensures the bound holds with high probability at least <span class="math notranslate nohighlight">\( 1-\delta \)</span>.</p>
<div class="dropdown admonition">
<p class="admonition-title">How did we obtain <span class="math notranslate nohighlight">\(\sqrt{\frac{\log(1/\delta)}{2m}}\)</span></p>
<p>The term <span class="math notranslate nohighlight">\( \sqrt{\frac{\log(1/\delta)}{2m}} \)</span> is a common term that arises in generalization bounds in the context of PAC learning, especially when using concentration inequalities like Hoeffding’s inequality.</p>
<p>Let’s derive this term using Hoeffding’s inequality:</p>
<p><strong>Hoeffding’s Inequality</strong>:
Given independent random variables <span class="math notranslate nohighlight">\( X_1, X_2, \ldots, X_m \)</span> that are bounded such that <span class="math notranslate nohighlight">\( a_i \leq X_i \leq b_i \)</span> for all <span class="math notranslate nohighlight">\( i \)</span>, let <span class="math notranslate nohighlight">\( S_m = \sum_{i=1}^{m} X_i \)</span>. Then, for any <span class="math notranslate nohighlight">\( t &gt; 0 \)</span>:</p>
<div class="math notranslate nohighlight">
\[ P\left( \frac{S_m}{m} - E\left[\frac{S_m}{m}\right] &gt; t \right) \leq \exp\left(-\frac{2m^2t^2}{\sum_{i=1}^{m}(b_i - a_i)^2}\right) \]</div>
<p>Now, consider the case of binary classification where each <span class="math notranslate nohighlight">\( X_i \)</span> represents the error of a hypothesis on a single example, so <span class="math notranslate nohighlight">\( X_i \in \{0, 1\} \)</span>. This means <span class="math notranslate nohighlight">\( a_i = 0 \)</span> and <span class="math notranslate nohighlight">\( b_i = 1 \)</span> for all <span class="math notranslate nohighlight">\( i \)</span>. The bound becomes:</p>
<div class="math notranslate nohighlight">
\[ P\left( \frac{S_m}{m} - E\left[\frac{S_m}{m}\right] &gt; t \right) \leq \exp(-2mt^2) \]</div>
<p>Setting the right-hand side to <span class="math notranslate nohighlight">\( \delta \)</span> (our desired confidence level) and solving for <span class="math notranslate nohighlight">\( t \)</span> gives:</p>
<div class="math notranslate nohighlight">
\[ \delta = \exp(-2mt^2) \]</div>
<div class="math notranslate nohighlight">
\[ \Rightarrow \log(1/\delta) = 2mt^2 \]</div>
<div class="math notranslate nohighlight">
\[ \Rightarrow t = \sqrt{\frac{\log(1/\delta)}{2m}} \]</div>
<p>This <span class="math notranslate nohighlight">\( t \)</span> is the amount by which the empirical mean (observed average error on the training set) can deviate from the expected mean (true error) with high probability.</p>
<p>In the context of PAC learning, this term provides a bound on the difference between the empirical risk (error on the training set) and the true risk (expected error on the entire distribution) of a hypothesis. The bound ensures that the empirical risk is a good approximation of the true risk with high probability.</p>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>True Risk of a Family</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[\hat{f}_m] \leq 2 \sup_{f \in \mathcal{F}} |\hat{R}[f] - R[f]| + R[f^*] \]</div>
<p>This equation bounds the true risk of the empirical risk minimizer <span class="math notranslate nohighlight">\( \hat{f}_m \)</span> (the function that has the lowest empirical risk in the hypothesis class). The term <span class="math notranslate nohighlight">\( \sup_{f \in \mathcal{F}} |\hat{R}[f] - R[f]| \)</span> represents the maximum difference between empirical and true risk over all functions in the hypothesis class <span class="math notranslate nohighlight">\( \mathcal{F} \)</span>. This difference is often referred to as the “uniform divergence.” The term <span class="math notranslate nohighlight">\( R[f^*] \)</span> is the risk of the best possible function in the hypothesis class.</p>
<div class="dropdown admonition">
<p class="admonition-title">Why is it 2sup</p>
<p>The term <span class="math notranslate nohighlight">\( 2 \sup_{f \in \mathcal{F}} |\hat{R}[f] - R[f]| \)</span> that you’re referring to is related to the generalization bound in the context of PAC learning. Let’s break it down:</p>
<p>The term <span class="math notranslate nohighlight">\( \sup_{f \in \mathcal{F}} |\hat{R}[f] - R[f]| \)</span> represents the maximum difference between the empirical risk (risk on the training set) and the true risk (expected risk on the entire distribution) over all functions in the hypothesis class <span class="math notranslate nohighlight">\( \mathcal{F} \)</span>. This difference is often referred to as the “uniform divergence” or “uniform deviation.”</p>
<p>The factor of 2 in front of the supremum is a result of symmetrization arguments used in the derivation of generalization bounds. Here’s a high-level idea:</p>
<p>When deriving generalization bounds, one common technique is to introduce a set of independent and identically distributed (i.i.d.) “ghost” or “shadow” samples. These are hypothetical samples that come from the same distribution as the training data but are not actually observed. By comparing the performance of hypotheses on the actual training data and the ghost samples, one can derive bounds on the generalization error.</p>
<p>The introduction of ghost samples effectively doubles the deviation between the empirical risk and the true risk, leading to the factor of 2 in the bound. This symmetrization technique simplifies the analysis and is a common step in the derivation of generalization bounds in statistical learning theory.</p>
<p>In essence, the factor of 2 accounts for the worst-case scenario in which the empirical risk underestimates the true risk by the maximum possible amount, and it ensures that the derived bound is valid with high probability across all functions in the hypothesis class.</p>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>True Risk of Finite Families</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[\hat{f}_m] \leq \hat{R}[f] + \sqrt{\frac{\log |\mathcal{F}| + \log(1/\delta)}{m}} \]</div>
<p>For a finite hypothesis class <span class="math notranslate nohighlight">\( \mathcal{F} \)</span>, this equation provides a bound on the true risk of the empirical risk minimizer <span class="math notranslate nohighlight">\( \hat{f}_m \)</span>. The term <span class="math notranslate nohighlight">\( \sqrt{\frac{\log |\mathcal{F}| + \log(1/\delta)}{m}} \)</span> is a confidence interval that accounts for the size of the hypothesis class and ensures the bound holds with high probability.</p>
<p>The derivation of these bounds involves a combination of probability theory (especially concentration inequalities like Hoeffding’s inequality or the union bound) and combinatorial arguments related to the complexity of the hypothesis class (like the VC dimension). The goal of these bounds is to provide theoretical guarantees on the performance of learning algorithms, helping to understand their generalization capabilities.</p>
<div class="dropdown admonition">
<p class="admonition-title">How is <span class="math notranslate nohighlight">\(\sqrt{\frac{\log |F| + \log(1/\delta)}{m}}\)</span> calculated</p>
<p>This term is a generalization bound often associated with the true risk of finite hypothesis classes in the Probably Approximately Correct (PAC) learning framework. This bound is derived using concentration inequalities, especially the union bound and Hoeffding’s inequality. Let’s break down its derivation:</p>
<ol class="arabic simple">
<li><p><strong>Hoeffding’s Inequality for a Single Hypothesis</strong>:
For a single hypothesis <span class="math notranslate nohighlight">\( f \)</span> from a finite hypothesis class <span class="math notranslate nohighlight">\( F \)</span>, the difference between its empirical risk <span class="math notranslate nohighlight">\( \hat{R}[f] \)</span> (observed on the training set) and its true risk <span class="math notranslate nohighlight">\( R[f] \)</span> (over the entire distribution) can be bounded using Hoeffding’s inequality:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ P\left( R[f] &gt; \hat{R}[f] + \sqrt{\frac{\log(2/\delta)}{2m}} \right) &lt; \delta \]</div>
<p>This says that the true risk of <span class="math notranslate nohighlight">\( f \)</span> exceeds its empirical risk by more than <span class="math notranslate nohighlight">\( \sqrt{\frac{\log(2/\delta)}{2m}} \)</span> with probability less than <span class="math notranslate nohighlight">\( \delta \)</span>.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Union Bound for Multiple Hypotheses</strong>:
Since <span class="math notranslate nohighlight">\( F \)</span> is a finite hypothesis class, we can apply the union bound to account for all hypotheses in <span class="math notranslate nohighlight">\( F \)</span>. The union bound states that the probability of any one of several events occurring is at most the sum of their individual probabilities.</p></li>
</ol>
<p>Let’s say we want the above bound to hold for all hypotheses in <span class="math notranslate nohighlight">\( F \)</span> simultaneously with a confidence of at least <span class="math notranslate nohighlight">\( 1-\delta \)</span>. Using the union bound:</p>
<div class="math notranslate nohighlight">
\[ P\left( \bigcup_{f \in F} \left( R[f] &gt; \hat{R}[f] + \sqrt{\frac{\log(2/\delta')}{2m}} \right) \right) &lt; |F| \cdot \delta' \]</div>
<p>Where <span class="math notranslate nohighlight">\( \delta' \)</span> is the probability for a single hypothesis. If we set <span class="math notranslate nohighlight">\( |F| \cdot \delta' = \delta \)</span>, then <span class="math notranslate nohighlight">\( \delta' = \frac{\delta}{|F|} \)</span>.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Combining the Bounds</strong>:
Substituting <span class="math notranslate nohighlight">\( \delta' \)</span> into the bound from Hoeffding’s inequality:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[f] &gt; \hat{R}[f] + \sqrt{\frac{\log(2|F|/\delta)}{2m}} \]</div>
<p>This bound ensures that the true risk of every hypothesis in <span class="math notranslate nohighlight">\( F \)</span> is close to its empirical risk with high probability.</p>
<ol class="arabic simple" start="4">
<li><p><strong>Simplifying</strong>:
For clarity and simplicity, the bound is often written as:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ R[f] \leq \hat{R}[f] + \sqrt{\frac{\log |F| + \log(1/\delta)}{m}} \]</div>
<p>This is the generalization bound for finite hypothesis classes in the PAC framework, and it provides a theoretical guarantee on the performance of a learning algorithm on unseen data based on its performance on the training data.</p>
</div>
</section>
</section>
<hr class="docutils" />
<section id="vc-dimension">
<h2>VC dimension<a class="headerlink" href="#vc-dimension" title="Permalink to this heading">#</a></h2>
<section id="def">
<h3>Def<a class="headerlink" href="#def" title="Permalink to this heading">#</a></h3>
<p>Vapnik-Chervonenkis (VC) Dimension
The maximum number of points that can be shattered by H is called the Vapnik-Chervonenkis dimension of H.
VC dimension is one measure that characterizes the expressive power or capacity of a hypothesis class</p>
<p>The VC (Vapnik-Chervonenkis) dimension is a fundamental concept in statistical learning theory. It provides a measure of the capacity or complexity of a hypothesis class. Specifically, the VC dimension gives an upper bound on the number of training samples that a hypothesis class can shatter, which in turn provides insights into the generalization ability of a learning algorithm.</p>
<p>Here’s a more detailed breakdown:</p>
<ol class="arabic simple">
<li><p><strong>Shattering</strong>: A hypothesis class <span class="math notranslate nohighlight">\( H \)</span> is said to “shatter” a set of points if, for every possible labeling of those points (binary labels, typically 0 or 1), there exists some hypothesis in <span class="math notranslate nohighlight">\( H \)</span> that perfectly separates the points according to that labeling.</p></li>
<li><p><strong>VC Dimension</strong>: The VC dimension of a hypothesis class <span class="math notranslate nohighlight">\( H \)</span> is the largest number of points that <span class="math notranslate nohighlight">\( H \)</span> can shatter. If <span class="math notranslate nohighlight">\( H \)</span> can shatter an arbitrarily large number of points, then its VC dimension is said to be infinite.</p></li>
<li><p><strong>Implications for Learning</strong>:</p>
<ul class="simple">
<li><p>A higher VC dimension indicates that the hypothesis class is more complex and can represent a wider variety of functions.</p></li>
<li><p>However, a higher VC dimension also means that the hypothesis class can easily overfit to the training data. This is because a more complex hypothesis class can fit the training data in many ways, including fitting to the noise in the data.</p></li>
<li><p>The VC dimension plays a crucial role in deriving generalization bounds in machine learning. Specifically, it helps in determining how well a learning algorithm will perform on unseen data, given its performance on the training data.</p></li>
</ul>
</li>
<li><p><strong>Examples</strong>:</p>
<ul class="simple">
<li><p>For a hypothesis class of linear classifiers in 2D (lines in the plane), the VC dimension is 3. This is because any 3 non-collinear points can be shattered by lines in the plane, but 4 general points cannot be.</p></li>
<li><p>For a hypothesis class of linear classifiers in <span class="math notranslate nohighlight">\( d \)</span>-dimensions (hyperplanes), the VC dimension is <span class="math notranslate nohighlight">\( d + 1 \)</span>.</p></li>
</ul>
</li>
<li><p><strong>Relationship with PAC Learning</strong>: In the PAC (Probably Approximately Correct) learning framework, the VC dimension helps determine the number of samples required to learn a concept within a certain error bound. Specifically, if the VC dimension is finite, then the sample complexity for PAC learning is polynomial in terms of the VC dimension, the inverse of the desired error <span class="math notranslate nohighlight">\( \epsilon \)</span>, and the inverse of the confidence <span class="math notranslate nohighlight">\( \delta \)</span>.</p></li>
</ol>
<p>The VC dimension provides a bridge between the empirical performance of a learning algorithm (its performance on the training set) and its expected performance on unseen data, making it a foundational concept in understanding the generalization capabilities of learning algorithms.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>N data points can be classified in <span class="math notranslate nohighlight">\(2^N\)</span> ways</p>
</div>
<div class="admonition-what-is-vc-dimension-why-do-we-need-it admonition">
<p class="admonition-title">what is VC dimension why do we need it? </p>
<p>The VC (Vapnik-Chervonenkis) dimension is a measure of the capacity or complexity of a hypothesis class in machine learning. It quantifies the expressive power of a class of functions by determining the largest set of points that can be shattered (i.e., classified in all possible ways) by that class. A higher VC dimension indicates that the hypothesis class can represent more complex functions.</p>
</div>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<p><strong>Shattering</strong>
A set of N points is said to be shattered by a hypothesis space H, if there are hypothesis(h) in H that separates positive examples from the negative examples in all of the 2N possible ways.</p>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/vc1.png"><img alt="VC" class="bg-primary mb-1 align-center" src="_images/vc1.png" style="width: 800px;" /></a>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/vc2.png"><img alt="VC" class="bg-primary mb-1 align-center" src="_images/vc2.png" style="width: 800px;" /></a>
<ul class="simple">
<li><p>It may not be able to shatter every possible set of three points in 2 dimensions.</p></li>
<li><p>It is enough to find one set of three points that can be shattered.</p></li>
</ul>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/vc3.png"><img alt="VC" class="bg-primary mb-1 align-center" src="_images/vc3.png" style="width: 800px;" /></a>
<p>We can’t find a set of 4 points that can be shattered by a single straight line in 2D.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Hence, Maximum number of points in R2 that be shattered by straight line is 3</p>
</div>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/vc4.png"><img alt="VC" class="bg-primary mb-1 align-center" src="_images/vc4.png" style="width: 800px;" /></a>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>VCD (Axis Aligned Rectangle in R2)= 4</p>
<p>AKA Maximum number of points in R2 that be shattered by a square is 4</p>
</div>
</section>
<section id="addtional-resource">
<h3>Addtional Resource<a class="headerlink" href="#addtional-resource" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.cs.columbia.edu/~verma/classes/ml/ref/lec6_vc.pdf">VC Dimension and PAC</a></p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="review2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Review 2</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#toc">ToC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-equation">normal equation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#newton-raphston">Newton Raphston</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pca">PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-learning">PAC Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-learnability">PAC Learnability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pac-cont">PAC Cont.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vc-dimension">VC dimension</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#def">Def</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#addtional-resource">Addtional Resource</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>