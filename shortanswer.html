

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Short Answer Questions &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'shortanswer';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Short Answers" href="QNA.html" />
    <link rel="prev" title="Final Review Notes" href="comparisons.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to COMP90051
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="revision_progress.html">Revision Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="basics.html">Basic Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="resources.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="comparisons.html">Final Review Notes</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Short Answer Questions</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="QNA.html">Short Answers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week1.0.html">week1</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week1.1.html">Lecture 1.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.2.html">Lecture 2.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet2note.html">worksheet2note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week2.0.html">week2</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week2.1.html">Lecture 3.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.2.html">Lecture 4.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet3note.html">worksheet3note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week3.0.html">week3</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week3.1.html">Lecture 5. Regularization</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.2.html">Lecture 6. PAC Learning Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet4note.html">worksheet4note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week4.0.html">week4</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week4.1.html">Lecture 7. VC Theory</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.2.html">Lecture 8. Support Vector Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet5note.html">worksheet5note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week5.0.html">week5</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week5.1.html">Lecture 9. Kernel Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.2.html">Lecture 10. The Perceptron</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.3.html">Additional notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet6note.html">worksheet6note</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week6.0.html">week6</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week6.1.html">Lecture 11. Neural Network Fundamentals</a></li>
<li class="toctree-l2"><a class="reference internal" href="week6.2.html">Lecture 12.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week6.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week7.0.html">week7</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week7.1.html">Lecture 13. Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.2.html">Lecture 14. RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.3.html">Additional Resource</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week8.0.html">week8</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week8.1.html">Lecture 16 Graph Convolution Networks (Deep Learning After You Drop The Camera)</a></li>
<li class="toctree-l2"><a class="reference internal" href="week8.2.html">Lecture 16. Learning with expert advice</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week9.0.html">week9</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week9.1.html">Stochastic Multi-Armed Bandits (MABs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="week9.2.html">Bayesian regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet10note.html">Workshop 10: Multi-armed bandits notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week10.0.html">week10</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week10.1.html">Bayesian classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="week10.2.html">PGM Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="week10.3.html">Additional Notes -  More on Bayesian</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week11.0.html">week11</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week11.1.html">U-PGM</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.2.html">SVM assignment</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.3.html">Lecture 22. Inference on PGMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="asm1feedback.html">ASM2 feedback</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week12.0.html">week12</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week12.1.html">Lecture 22. Inference on PGMs Cont. &amp; Lecture 23. Gaussian Mixture Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="week12.2.html">Lecture 24. Subject Review and Exam Info</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="review.html">Review Notes</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="review0.html">Review 0</a></li>
<li class="toctree-l2"><a class="reference internal" href="review1.html">Review 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="review2.html">Review 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="review3.html">Review 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="review4.html">Review 4</a></li>
<li class="toctree-l2"><a class="reference internal" href="review5.html">Review 5</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fshortanswer.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/shortanswer.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Short Answer Questions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-and-deep-learning"><strong>1. Neural Networks and Deep Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#svms-and-kernel-methods"><strong>2. SVMs and Kernel Methods</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-and-probabilistic-models"><strong>3. Bayesian and Probabilistic Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-and-learning-theory"><strong>4. Optimization and Learning Theory</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous"><strong>5. Miscellaneous</strong></a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="short-answer-questions">
<h1>Short Answer Questions<a class="headerlink" href="#short-answer-questions" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<section id="neural-networks-and-deep-learning">
<h2><strong>1. Neural Networks and Deep Learning</strong><a class="headerlink" href="#neural-networks-and-deep-learning" title="Permalink to this heading">#</a></h2>
<p><strong>ProTip</strong>: Deep learning questions often touch on architectures, optimization techniques, and challenges like vanishing gradients. Familiarize yourself with common architectures like CNNs, RNNs, and attention mechanisms.</p>
<ul class="simple">
<li><p><strong>Question</strong>: In what respect is a recurrent neural network deep?</p>
<ul>
<li><p><strong>Answer</strong>: RNNs are “deep” in terms of <strong>time or sequence length</strong>. They process sequences and maintain a hidden state across time steps, allowing them to capture long-range dependencies in data.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: How does the value of the learning rate, <span class="math notranslate nohighlight">\( \eta \)</span> in SGD affect the training progress?</p>
<ul>
<li><p><strong>Answer</strong>: A <strong>high learning rate</strong> may cause oscillations and overshooting, potentially missing the minimum. A <strong>low learning rate</strong> might converge slowly or get stuck in local minima. An optimal learning rate allows steady convergence to a global or good local minimum.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Can training a deep neural network with tanh activation using backpropagation lead to the vanishing gradient problem? Why?</p>
<ul>
<li><p><strong>Answer</strong>: Yes, because the derivatives of the tanh function are in the range (0,1). For deep networks, multiplying these small derivatives during backpropagation can cause gradients to <strong>vanish</strong> and hinder weight updates in early layers.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Why are vanishing gradients a more pressing concern for recurrent neural networks (RNNs) than for other neural architectures?</p>
<ul>
<li><p><strong>Answer</strong>: RNNs process sequences and backpropagate errors through time steps. When gradients are small, they can <strong>vanish</strong> over many time steps, leading to long-range dependencies not being captured effectively and making early layers hard to train.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Explain how convolutional neural networks (CNNs) can produce translation invariant representations when applied to vector or matrix inputs.</p>
<ul>
<li><p><strong>Answer</strong>: CNNs use shared weights in their convolutional filters. This allows them to detect features regardless of their position in the input, leading to <strong>translation invariance</strong>. Pooling layers further enhance this property by reducing spatial dimensions.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: A stochastic gradient descent (SGD) training step for a convolutional neural network (CNN) involves loading a training image into memory: as we need prediction, loss, and gradients for this image. Graph convolutional networks (GCNs) can involve very large graphs — too large to load into memory all at once: how can we reduce the memory footprint of training over the entire graph, when training a GCN?</p>
<ul>
<li><p><strong>Answer</strong>: Use <strong>mini-batch training</strong> with <strong>neighbor sampling</strong> or <strong>graph sampling</strong> techniques. Instead of loading the entire graph, load and process smaller subgraphs or neighborhoods iteratively.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Both convolutional networks (CNNs) and recurrent networks (RNNs) can be applied to sequence inputs. Explain the key benefit that RNNs have over CNNs for this type of input.</p>
<ul>
<li><p><strong>Answer</strong>: RNNs are designed to handle sequences and capture temporal dependencies by maintaining a memory (hidden state) from previous steps, making them more suited for sequential data compared to CNNs.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Explain in words how Attention can be used to allow for neural models to process dynamic sized inputs.</p>
<ul>
<li><p><strong>Answer</strong>: Attention mechanisms weight input elements differently, allowing the model to “focus” on parts of the input that are more relevant for a particular task. This dynamic weighting enables models to handle variable-sized inputs efficiently.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
</section>
<section id="svms-and-kernel-methods">
<h2><strong>2. SVMs and Kernel Methods</strong><a class="headerlink" href="#svms-and-kernel-methods" title="Permalink to this heading">#</a></h2>
<p><strong>ProTip</strong>: For SVMs, understanding the distinction between hard-margin and soft-margin, the role of the kernel trick, and the primal vs. dual formulations can be particularly beneficial.</p>
<ul class="simple">
<li><p><strong>Question</strong>: For the hard-margin support vector machine, data points that are support vectors have a margin of 1 from the decision boundary. Explain how this can be the case even when support vectors are further than 1 unit away from the decision boundary in Euclidean space.</p>
<ul>
<li><p><strong>Answer</strong>: The margin of 1 refers to the <strong>distance in the transformed feature space</strong>, not the original space. Due to the transformation (e.g., by a kernel), points may appear closer/farther in the transformed space than in the original Euclidean space.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: When is it better to use the primal program for training an SVM with a quadratic kernel?</p>
<ul>
<li><p><strong>Answer</strong>: When the <strong>number of features (d) is less than the number of samples (n)</strong>. The complexity of the primal is related to the number of features, while the dual’s complexity is related to the number of samples.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Suppose you have trained a soft-margin support vector machine (SVM) with a RBF kernel, and the performance is very good on the training set while very poor on the validation set. How will you change the hyperparameters of the SVM to improve the performance of the validation set?</p>
<ul>
<li><p><strong>Answer</strong>: <strong>1. Increase the regularization parameter <span class="math notranslate nohighlight">\( C \)</span></strong>: A larger <span class="math notranslate nohighlight">\( C \)</span> will impose more regularization, preventing overfitting. <strong>2. Adjust the RBF kernel parameter <span class="math notranslate nohighlight">\( \sigma \)</span> or <span class="math notranslate nohighlight">\( \gamma \)</span></strong>: Increase the width of the Gaussian function, making the decision boundary smoother.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Weak duality guarantees that a primal optimum always upper bounds a dual optimum. How can we tell that the support vector machine’s primal and dual optima are always equal?</p>
<ul>
<li><p><strong>Answer</strong>: SVM’s optimization problem satisfies the <strong>Karush-Kuhn-Tucker (KKT) conditions</strong>, which ensure strong duality. Thus, the primal and dual optima are equal when these conditions are met.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: List two general strategies for proving that a learning algorithm can be kernelized.</p>
<ul>
<li><p><strong>Answer</strong>: <strong>1. Representer Theorem</strong>: Demonstrate that the solution to the optimization problem can be expressed as a linear combination of kernel evaluations with training data. <strong>2. Dual Formulation</strong>: Show that the problem’s dual formulation only involves dot products between data points, which can then be replaced by kernel functions.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: For a support vector machine with a quadratic kernel, in what situation, if any, would it be better to use the primal program instead of the dual program for training?</p>
<ul>
<li><p><strong>Answer</strong>: When the <strong>number of features (d) is less than the number of samples (n)</strong>. In such cases, the primal problem might be more efficient to solve than the dual.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
</section>
<section id="bayesian-and-probabilistic-models">
<h2><strong>3. Bayesian and Probabilistic Models</strong><a class="headerlink" href="#bayesian-and-probabilistic-models" title="Permalink to this heading">#</a></h2>
<p><strong>ProTip</strong>: Understanding the distinction between the frequentist and Bayesian perspectives, as well as how evidence, likelihood, and priors interact, can be pivotal for this category.</p>
<ul class="simple">
<li><p><strong>Question</strong>: How does Thompson sampling achieve exploration of infrequently played arms in multi-armed bandit learning?</p>
<ul>
<li><p><strong>Answer</strong>: Thompson sampling samples from the posterior distribution over each arm’s reward. By occasionally sampling optimistic estimates, even for infrequently played arms, it ensures <strong>exploration</strong> of all arms over</p></li>
</ul>
</li>
</ul>
<p>time.</p>
<ul class="simple">
<li><p><strong>Question</strong>: Why compute evidence for a Bayesian posterior but ignore it for MAP estimation?</p>
<ul>
<li><p><strong>Answer</strong>: The evidence, which is the denominator in Bayes’ rule, acts as a normalizing constant to ensure the posterior is a valid probability distribution. For MAP estimation, we’re interested in the <strong>mode</strong> of the posterior, so we can ignore the evidence since it doesn’t affect the location of the mode.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: How does the expectation maximization algorithm relate to the maximum likelihood estimate?</p>
<ul>
<li><p><strong>Answer</strong>: EM is used to find the <strong>maximum likelihood estimates</strong> (MLE) of parameters when the data has missing or hidden variables. It iteratively estimates the hidden variables (E-step) and then optimizes the parameters (M-step).</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Can the joint probability distribution described by any undirected probabilistic graphical model be expressed as a directed probabilistic graphical model? Explain why or why not.</p>
<ul>
<li><p><strong>Answer</strong>: Not always. Some factorizations and independence assumptions in undirected models (Markov random fields) may not have a straightforward equivalent in directed models (Bayesian networks) and vice versa.
<strong>ProTip</strong>: Understanding the distinction between the frequentist and Bayesian perspectives, as well as how evidence, likelihood, and priors interact, can be pivotal for this category.</p></li>
</ul>
</li>
</ul>
<p><strong>Questions</strong>:
<em>(Including the previously listed ones and any additional related ones)</em></p>
<ul class="simple">
<li><p><strong>Question</strong>: For Bayesian linear regression (as presented in class) explain how the posterior variance can vary for different test points.</p>
<ul>
<li><p><strong>Answer</strong>: The posterior variance depends on the test input <span class="math notranslate nohighlight">\( x_* \)</span> and captures our uncertainty about the prediction. It will be higher for test points far from training data and lower for points close to training data, reflecting increased confidence.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
</section>
<section id="optimization-and-learning-theory">
<h2><strong>4. Optimization and Learning Theory</strong><a class="headerlink" href="#optimization-and-learning-theory" title="Permalink to this heading">#</a></h2>
<p><strong>ProTip</strong>: Grasp the distinction between different optimization techniques like gradient descent, RMSProp, Adagrad, and coordinate descent. Familiarity with concepts like VC-dimension, PAC learning, and bounds can be beneficial.</p>
<ul class="simple">
<li><p><strong>Question</strong>: How does the value of the learning rate, <span class="math notranslate nohighlight">\( \eta \)</span> in SGD affect the training progress?</p>
<ul>
<li><p><strong>Answer</strong>: A <strong>high learning rate</strong> may cause oscillations and overshooting, potentially missing the minimum. A <strong>low learning rate</strong> might converge slowly or get stuck in local minima. An optimal learning rate allows steady convergence to a global or good local minimum.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Probably approximately correct (PAC) learning theory aims to upper bound the true risk <span class="math notranslate nohighlight">\( R[f] \)</span> of a learned model <span class="math notranslate nohighlight">\( f \in F \)</span> by the empirical risk <span class="math notranslate nohighlight">\( \hat{R}[f] \)</span> of that model plus an error term that might involve a sample size <span class="math notranslate nohighlight">\( m \)</span>, VC-dimension <span class="math notranslate nohighlight">\( VC(F) \)</span>, confidence parameter <span class="math notranslate nohighlight">\( \delta \)</span>, etc. Why do PAC bounds hold only “with high probability <span class="math notranslate nohighlight">\( 1 - \delta \)</span>”, and not deterministically?</p>
<ul>
<li><p><strong>Answer</strong>: PAC bounds are probabilistic because they account for the <strong>random nature</strong> of drawing a finite sample from a distribution. The bound guarantees that the true risk is close to the empirical risk for most samples, but there’s a small probability <span class="math notranslate nohighlight">\( \delta \)</span> where this might not hold.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Why might VC-dimension based PAC learning theory lead to very large or impractical risk bounds for deep neural networks with non-linear activations and multiple hidden layers?</p>
<ul>
<li><p><strong>Answer</strong>: Deep neural networks with non-linear activations have a very <strong>high VC-dimension</strong>, which reflects their capacity to fit a large variety of functions. Using VC-dimension in PAC bounds for such models can result in very large or overly pessimistic bounds, making them less informative or useful.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: If a training problem allows a closed form solution, would there be any advantage to using an iterative gradient based optimization method?</p>
<ul>
<li><p><strong>Answer</strong>: While closed-form solutions are computationally efficient, gradient-based methods can be more <strong>flexible</strong>, allowing for regularization, easier integration with other components, or scalability to large datasets.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Gradient descent is typically preferred over coordinate descent. Given this, why is coordinate descent used in the Expectation Maximisation algorithm?</p>
<ul>
<li><p><strong>Answer</strong>: Coordinate descent can be more efficient in situations where optimizing one parameter at a time (holding others fixed) has a closed-form solution or is computationally simpler, as is often the case in the E-step and M-step of the Expectation Maximisation algorithm.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Conjugacy between the likelihood and prior is very important when using Bayesian models. However, conjugacy is not critical when using the maximum a posteriori (MAP) estimator. Explain why this is the case.</p>
<ul>
<li><p><strong>Answer</strong>: In MAP estimation, we’re interested in finding the mode (peak) of the posterior distribution. Conjugacy ensures the posterior remains in the same family as the prior, which simplifies computations. However, for finding the mode, conjugacy isn’t essential as we’re not explicitly computing the full posterior distribution.</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
</section>
<section id="miscellaneous">
<h2><strong>5. Miscellaneous</strong><a class="headerlink" href="#miscellaneous" title="Permalink to this heading">#</a></h2>
<p><strong>ProTip</strong>: These questions touch on varied aspects of machine learning, from model types to optimization strategies. Ensure a broad understanding of fundamental ML concepts.</p>
<p><strong>Questions</strong>:</p>
<ul class="simple">
<li><p><strong>Question</strong>: Describe something that can go wrong specifically with gradient descent while training a learner. Describe one approach to mitigating the problem.</p>
<ul>
<li><p><strong>Answer</strong>: Gradient descent can get stuck in local minima or saddle points, especially in non-convex optimization landscapes. One approach to mitigate this is to use <strong>momentum</strong> or optimization techniques like RMSProp and Adam, which can help navigate such challenging terrains.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: Explain why training examples are weighted within the AdaBoost algorithm.</p>
<ul>
<li><p><strong>Answer</strong>: In AdaBoost, examples are weighted to give more emphasis to the misclassified examples in previous rounds. This ensures that subsequent weak learners focus more on these harder examples, making the ensemble more robust.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: How are generative models different from discriminative models?</p>
<ul>
<li><p><strong>Answer</strong>: Generative models learn the joint probability <span class="math notranslate nohighlight">\( P(X, Y) \)</span> and model how the data is generated. Discriminative models learn the conditional probability <span class="math notranslate nohighlight">\( P(Y|X) \)</span> and focus on separating the classes.</p></li>
</ul>
</li>
<li><p><strong>Question</strong>: In words, define the tree width of a directed graph <span class="math notranslate nohighlight">\( G \)</span>.</p>
<ul>
<li><p><strong>Answer</strong>: Tree width measures the “tree-likeness” of a graph. A smaller tree width indicates the graph is closer to being a tree. Specifically, it’s the size of the largest set in a tree decomposition of the graph minus one.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="comparisons.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Final Review Notes</p>
      </div>
    </a>
    <a class="right-next"
       href="QNA.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Short Answers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-and-deep-learning"><strong>1. Neural Networks and Deep Learning</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#svms-and-kernel-methods"><strong>2. SVMs and Kernel Methods</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-and-probabilistic-models"><strong>3. Bayesian and Probabilistic Models</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-and-learning-theory"><strong>4. Optimization and Learning Theory</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous"><strong>5. Miscellaneous</strong></a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>