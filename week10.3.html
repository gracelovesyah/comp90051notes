

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Additional Notes - More on Bayesian &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week10.3';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="COMP90051 Workshop 11" href="worksheet11.html" />
    <link rel="prev" title="PGM Representation" href="week10.2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to COMP90051
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="revision_progress.html">Revision Progress</a></li>
<li class="toctree-l1"><a class="reference internal" href="basics.html">Basic Concepts</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week1.0.html">week1</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week1.1.html">Lecture 1.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.2.html">Lecture 2.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week1.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week2.0.html">week2</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week2.1.html">Lecture 3.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.2.html">Lecture 4.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week2.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week3.0.html">week3</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week3.1.html">Lecture 5.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.2.html">Lecture 6.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week3.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week4.0.html">week4</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week4.1.html">Lecture 7.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.2.html">Lecture 8.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week4.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week5.0.html">week5</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week5.1.html">Lecture 9.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.2.html">Lecture 10.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week5.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week6.0.html">week6</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week6.1.html">Lecture 11.</a></li>
<li class="toctree-l2"><a class="reference internal" href="week6.2.html">week6 lec2</a></li>
<li class="toctree-l2"><a class="reference internal" href="week6.3.html">Additional notes</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week7.0.html">week7</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week7.1.html">Lecture 13. Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.2.html">Lecture 14. RNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="week7.3.html">Additional Resource</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week8.0.html">week8</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week8.1.html">Lecture 16 Graph Convolution Networks (Deep Learning After You Drop The Camera)</a></li>
<li class="toctree-l2"><a class="reference internal" href="week8.2.html">Lecture 16. Learning with expert advice</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week9.0.html">week9</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week9.1.html">Stochastic Multi-Armed Bandits (MABs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="week9.2.html">Bayesian regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet10.html">Workshop 10: Multi-armed bandits</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet10note.html">Workshop 10: Multi-armed bandits notes</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="week10.0.html">week10</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="week10.1.html">wk10 notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="week10.2.html">PGM Representation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Additional Notes -  More on Bayesian</a></li>
<li class="toctree-l2"><a class="reference internal" href="worksheet11.html">COMP90051 Workshop 11</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week11.0.html">week11</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week11.1.html">U-PGM</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.2.html">SVM assignment</a></li>
<li class="toctree-l2"><a class="reference internal" href="week11.3.html">Lecture 22. Inference on PGMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="asm1feedback.html">ASM2 feedback</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="week12.0.html">week12</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="week12.1.html">Lecture 22. Inference on PGMs Cont. &amp; Lecture 23. Gaussian Mixture Models</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="review.html">Review Notes</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="review0.html">Review 0</a></li>
<li class="toctree-l2"><a class="reference internal" href="review1.html">Review 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="review2.html">Review 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="review3.html">Review 3</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fweek10.3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/week10.3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Additional Notes -  More on Bayesian</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-bayesian">More on Bayesian</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-resources">Useful Resources</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distributions">Joint Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-and-posterior">Prior and Posterior</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="additional-notes-more-on-bayesian">
<h1>Additional Notes -  More on Bayesian<a class="headerlink" href="#additional-notes-more-on-bayesian" title="Permalink to this heading">#</a></h1>
<section id="more-on-bayesian">
<h2>More on Bayesian<a class="headerlink" href="#more-on-bayesian" title="Permalink to this heading">#</a></h2>
<p>This note is completed with the assistance of <a class="reference external" href="https://chat.openai.com/c/7887d9af-ce1c-4551-8f91-c576874448be">ChatGPT</a></p>
<section id="useful-resources">
<h3>Useful Resources<a class="headerlink" href="#useful-resources" title="Permalink to this heading">#</a></h3>
<p><strong>Bayesian Regression and Inference Youtube Series</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=VQ1dxoopfEI&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn">The Battle of Polynomials | Towards Bayesian Regression</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=NyH9K3stvP8&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=2">Maximum Likelihood Estimation - THINK PROBABILITY FIRST!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=dW_IwqNnapM&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=3">Bayesian Curve Fitting - Your First Baby Steps!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=xjLqawhT3bY&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=4">Sum Rule, Product Rule, Joint &amp; Marginal Probability - CLEARLY EXPLAINED with EXAMPLES!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=Kz7YbxHkVI0&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=5">Posterior Predictive Distribution - Proper Bayesian Treatment!</a>üåüüåüüåü</p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=1u_5ZNFAItc&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=6">How to Read &amp; Make Graphical Models?</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=pG0S2u188bg&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=7">Conjugate Prior - Use &amp; Limitations CLEARLY EXPLAINED!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=uUqK_me5xUo&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=8">Monte Carlo Methods - VISUALLY EXPLAINED!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=CIe869Rce2k&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=9">Markov Chains - VISUALLY EXPLAINED + History!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=oX2wIGSn4jY&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=10">Metropolis-Hastings - VISUALLY EXPLAINED!</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=-C8PGgnF1yg&amp;list=PLivJwLo9VCUISiuiRsbm5xalMbIwOHOOn&amp;index=11">Probabilistic Programming - FOUNDATIONS &amp; COMPREHENSIVE REVIEW!</a></p></li>
</ul>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/bayesian2.png"><img alt="b2" class="bg-primary mb-1 align-center" src="_images/bayesian2.png" style="width: 800px;" /></a>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/bayesian1.png"><img alt="b1" class="bg-primary mb-1 align-center" src="_images/bayesian1.png" style="width: 800px;" /></a>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/bayesian3.png"><img alt="b3" class="bg-primary mb-1 align-center" src="_images/bayesian3.png" style="width: 800px;" /></a>
</section>
<section id="joint-distributions">
<h3>Joint Distributions<a class="headerlink" href="#joint-distributions" title="Permalink to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>Why Bayesian Approaches Often Require Working with Joint Distributions:</strong></p>
<ul class="simple">
<li><p><strong>Bayes‚Äô Theorem:</strong> The foundation of Bayesian inference is Bayes‚Äô theorem, which relates the joint distribution and the conditional distributions of the variables involved. Specifically, it states:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ P(A|B) = \frac{P(B|A) \times P(A)}{P(B)} \]</div>
<p>Here, <span class="math notranslate nohighlight">\( P(A|B) \)</span> is the posterior, <span class="math notranslate nohighlight">\( P(B|A) \)</span> is the likelihood, <span class="math notranslate nohighlight">\( P(A) \)</span> is the prior, and <span class="math notranslate nohighlight">\( P(B) \)</span> is the evidence (or marginal likelihood). The evidence is often computed by summing or integrating over the joint distribution of <span class="math notranslate nohighlight">\( A \)</span> and <span class="math notranslate nohighlight">\( B \)</span>.</p>
<ul class="simple">
<li><p><strong>Updating Beliefs:</strong> Bayesian approaches are all about updating prior beliefs with new data to get a posterior belief. This updating process inherently involves working with the joint distribution of the data and the parameters.</p></li>
<li><p><strong>Predictive Distributions:</strong> To make predictions in a Bayesian framework, we often need to integrate out the parameters over their joint distribution with the data.</p></li>
</ul>
</li>
<li><p><strong>Why Choosing Appropriate Priors is Crucial in Bayesian Modeling:</strong></p>
<ul class="simple">
<li><p><strong>Influence on the Posterior:</strong> The prior represents our belief about a parameter before observing any data. If we have a strong prior belief (informative prior), it can significantly influence the posterior, especially if the amount of data is small. Conversely, if we have little prior knowledge, we might choose a non-informative or weak prior, letting the data play a more significant role in shaping the posterior.</p></li>
<li><p><strong>Regularization:</strong> Priors can act as regularizers. For instance, a prior that favors smaller values of a parameter can prevent overfitting in a model, similar to L1 or L2 regularization in non-Bayesian contexts.</p></li>
<li><p><strong>Incorporating Expert Knowledge:</strong> One of the strengths of Bayesian modeling is the ability to incorporate expert knowledge or beliefs into the model through the prior. This can be especially valuable in domains where data is scarce or expensive to obtain.</p></li>
<li><p><strong>Model Identifiability:</strong> In some cases, without an appropriate prior, a model might be non-identifiable, meaning there could be multiple parameter values that explain the data equally well. A prior can help in pinning down a unique or more plausible solution.</p></li>
<li><p><strong>Computational Stability:</strong> Priors can also aid in the computational stability of Bayesian inference algorithms, especially in high-dimensional parameter spaces.</p></li>
</ul>
</li>
</ol>
<p>In summary, Bayesian approaches inherently involve the manipulation and computation with joint distributions, as they provide a comprehensive view of the relationships between variables and are essential for updating beliefs and making predictions. Priors, on the other hand, are a defining feature of Bayesian modeling, allowing for the incorporation of prior beliefs and playing a crucial role in determining the behavior and properties of the resulting posterior distributions.</p>
</section>
<section id="prior-and-posterior">
<h3>Prior and Posterior<a class="headerlink" href="#prior-and-posterior" title="Permalink to this heading">#</a></h3>
<p>Updating prior beliefs is at the heart of Bayesian inference. The process involves using Bayes‚Äô theorem to combine our prior beliefs (prior distribution) with new evidence (data) to obtain an updated belief (posterior distribution). Here‚Äôs a step-by-step breakdown:</p>
<div class="dropdown admonition">
<p class="admonition-title">Prior represents?</p>
<p>Our beliefs about the parameter(s) P(Œ∏) before observing any data.</p>
</div>
<ol class="arabic simple">
<li><p><strong>Start with a Prior Distribution:</strong></p>
<ul class="simple">
<li><p>The prior distribution, denoted <span class="math notranslate nohighlight">\( P(\theta) \)</span>, represents our beliefs about the parameter(s) <span class="math notranslate nohighlight">\( \theta \)</span> before observing any data. This can be based on previous studies, expert opinion, or other sources of information.</p></li>
</ul>
</li>
<li><p><strong>Observe Data and Compute the Likelihood:</strong></p>
<ul class="simple">
<li><p>The likelihood, denoted <span class="math notranslate nohighlight">\( P(D|\theta) \)</span>, tells us how probable our observed data <span class="math notranslate nohighlight">\( D \)</span> is, given different values of the parameter(s) <span class="math notranslate nohighlight">\( \theta \)</span>. It quantifies how well different parameter values explain the observed data.</p></li>
</ul>
</li>
<li><p><strong>Apply Bayes‚Äô Theorem to Compute the Posterior Distribution:</strong></p>
<ul class="simple">
<li><p>The posterior distribution, denoted <span class="math notranslate nohighlight">\( P(\theta|D) \)</span>, represents our updated belief about the parameter(s) <span class="math notranslate nohighlight">\( \theta \)</span> after observing the data <span class="math notranslate nohighlight">\( D \)</span>. It‚Äôs computed using Bayes‚Äô theorem:</p></li>
</ul>
</li>
</ol>
<div class="math notranslate nohighlight">
\[ P(\theta|D) = \frac{P(D|\theta) \times P(\theta)}{P(D)} \]</div>
<ul class="simple">
<li><p>Here:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( P(\theta|D) \)</span> is the posterior.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(D|\theta) \)</span> is the likelihood.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(\theta) \)</span> is the prior.</p></li>
<li><p><span class="math notranslate nohighlight">\( P(D) \)</span> is the evidence or marginal likelihood. It‚Äôs a normalizing constant ensuring that the posterior distribution integrates (or sums) to 1. It‚Äôs computed by integrating (or summing) the numerator over all possible values of <span class="math notranslate nohighlight">\( \theta \)</span>:</p></li>
</ul>
</li>
</ul>
<div class="math notranslate nohighlight">
\[ P(D) = \int P(D|\theta) \times P(\theta) \, d\theta \]</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(For discrete parameters, the integral becomes a sum.)
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p><strong>Interpret the Posterior Distribution:</strong></p>
<ul class="simple">
<li><p>The posterior distribution combines the information from the prior and the data. If the data are strong and informative, the posterior will be more influenced by the likelihood. If the data are weak or scarce, the prior will play a more significant role in shaping the posterior.</p></li>
<li><p>The mode of the posterior distribution gives the Maximum A Posteriori (MAP) estimate of the parameter(s). If you‚Äôre interested in point estimates, the MAP is a common choice in Bayesian analysis.</p></li>
<li><p>The spread or width of the posterior distribution provides a measure of uncertainty about the parameter(s). Narrower distributions indicate higher certainty.</p></li>
</ul>
</li>
<li><p><strong>Iterative Updating:</strong></p>
<ul class="simple">
<li><p>As more data becomes available, the posterior distribution from the previous step can be used as the prior for the next update. This iterative updating is a powerful feature of Bayesian inference, allowing for continuous learning as new evidence is gathered.</p></li>
</ul>
</li>
</ol>
<p>In essence, the process of updating prior beliefs in Bayesian inference is a systematic way of combining prior knowledge with new data to refine our understanding of the underlying parameters or processes. This approach is particularly powerful in situations where data are limited or where incorporating expert knowledge is essential.</p>
<hr class="docutils" />
<p>The posterior distribution for <span class="math notranslate nohighlight">\( \mathbf{w} \)</span> given data <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{y} \)</span> is derived using Bayes‚Äô theorem. Let‚Äôs break down the derivation:</p>
<hr class="docutils" />
<p><strong>Bayes‚Äô Theorem:</strong></p>
<div class="math notranslate nohighlight">
\[ p(A|B) = \frac{p(B|A) \times p(A)}{p(B)} \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( p(A|B) \)</span> is the posterior probability of event A given event B.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(B|A) \)</span> is the likelihood of event B given event A.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(A) \)</span> is the prior probability of event A.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(B) \)</span> is the marginal likelihood or evidence.</p></li>
</ul>
<hr class="docutils" />
<p><strong>Applying to Bayesian Linear Regression:</strong></p>
<p>In the context of Bayesian linear regression:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( A \)</span> corresponds to the weights <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( B \)</span> corresponds to the observed data <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{y} \)</span>.</p></li>
</ul>
<p>Using Bayes‚Äô theorem, the posterior distribution for <span class="math notranslate nohighlight">\( \mathbf{w} \)</span> given <span class="math notranslate nohighlight">\( \mathbf{X} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{y} \)</span> is:</p>
<div class="math notranslate nohighlight">
\[ p(\mathbf{w}|\mathbf{X},\mathbf{y}) = \frac{p(\mathbf{y}|\mathbf{w},\mathbf{X}) \times p(\mathbf{w})}{p(\mathbf{y}|\mathbf{X})} \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( p(\mathbf{w}|\mathbf{X},\mathbf{y}) \)</span> is the posterior distribution of <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(\mathbf{y}|\mathbf{w},\mathbf{X}) \)</span> is the likelihood of observing <span class="math notranslate nohighlight">\( \mathbf{y} \)</span> given <span class="math notranslate nohighlight">\( \mathbf{w} \)</span> and <span class="math notranslate nohighlight">\( \mathbf{X} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(\mathbf{w}) \)</span> is the prior distribution of <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( p(\mathbf{y}|\mathbf{X}) \)</span> is the marginal likelihood or evidence, which can be computed as an integral over all possible values of <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ p(\mathbf{y}|\mathbf{X}) = \int p(\mathbf{y}|\mathbf{w},\mathbf{X}) \times p(\mathbf{w}) \, d\mathbf{w} \]</div>
<hr class="docutils" />
<p>In Bayesian linear regression, the prior <span class="math notranslate nohighlight">\( p(\mathbf{w}) \)</span> and the likelihood <span class="math notranslate nohighlight">\( p(\mathbf{y}|\mathbf{w},\mathbf{X}) \)</span> are typically chosen such that the posterior is analytically tractable. For example, if both the prior and the likelihood are Gaussian (normal) distributions, then the posterior will also be a Gaussian distribution. This choice simplifies the computation and allows for exact inference.</p>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>why would we write the denominator as the marginal likelihood?</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Answer</p>
<p>The term ‚Äúmarginal likelihood‚Äù or ‚Äúevidence‚Äù refers to the likelihood of the observed data, marginalized (or averaged) over all possible parameter values. In the context of Bayesian linear regression, the parameter we‚Äôre interested in is <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>, the vector of regression coefficients.</p>
<p>The reason we compute the marginal likelihood in this manner is rooted in the fundamentals of Bayesian inference. Let‚Äôs break it down:</p>
<ol class="arabic simple">
<li><p><strong>Definition of Marginal Likelihood:</strong> The marginal likelihood, <span class="math notranslate nohighlight">\( p(\mathbf{y}|\mathbf{X}) \)</span>, is the probability of the observed data <span class="math notranslate nohighlight">\( \mathbf{y} \)</span> given the features <span class="math notranslate nohighlight">\( \mathbf{X} \)</span>, averaged over all possible values of the parameters <span class="math notranslate nohighlight">\( \mathbf{w} \)</span>. Mathematically, it‚Äôs represented as:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[ p(\mathbf{y}|\mathbf{X}) = \int p(\mathbf{y}|\mathbf{w},\mathbf{X}) \times p(\mathbf{w}) \, d\mathbf{w} \]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Role in Bayes‚Äô Theorem:</strong> The marginal likelihood serves as a normalizing constant in Bayes‚Äô theorem. It ensures that the posterior distribution <span class="math notranslate nohighlight">\( p(\mathbf{w}|\mathbf{X},\mathbf{y}) \)</span> is a valid probability distribution that integrates (sums) to 1.</p></li>
<li><p><strong>Interpretation:</strong> The marginal likelihood provides a measure of how well the model (with its associated likelihood and prior) predicts the observed data, averaged over all possible parameter values. It plays a crucial role in model comparison, where models with higher marginal likelihoods are generally preferred.</p></li>
<li><p><strong>Computational Challenge:</strong> Directly computing the integral can be challenging, especially in high-dimensional spaces or with complex models. However, in some cases (like when using conjugate priors), the integral can be computed analytically. In other cases, approximation methods like Markov Chain Monte Carlo (MCMC) or Variational Inference might be used.</p></li>
</ol>
<p>In summary, the marginal likelihood is a fundamental concept in Bayesian statistics, representing the probability of the observed data under the model, averaged over all possible parameter values. It plays a key role in both the normalization of the posterior distribution and in model comparison.</p>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="week10.2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">PGM Representation</p>
      </div>
    </a>
    <a class="right-next"
       href="worksheet11.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">COMP90051 Workshop 11</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-on-bayesian">More on Bayesian</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#useful-resources">Useful Resources</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-distributions">Joint Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-and-posterior">Prior and Posterior</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      ¬© Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>